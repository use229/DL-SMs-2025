import torch.optim as optim
import xlrd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from KAN import CombinedMLP

def r_squared(x1, x2):
    # Calculate Total Sum of Squares (TSS)
    tss = np.sum((x2 - np.mean(x2)) ** 2)

    # Calculate Residual Sum of Squares (RSS)
    rss = np.sum((x2 - x1) ** 2)

    #  Calculate R^2
    r2 = 1 - (rss / tss)

    return r2

def calculate_accuracy(x1, x2):
    x1=x1.transpose(0,1)
    x2 = x2.transpose(0, 1)
    a = np.abs(x1-x2)
    b = x2
    acc = 1-np.mean(np.divide(a, b, out=np.zeros_like(a), where=b!=0))
    return acc
    #return r_squared(x1[0], x2[0]),r_squared(x1[1], x2[1])


def calculate_error(x1, x2):
    a = np.abs(x1-x2)
    b = x2
    e = np.mean(np.divide(a, b, out=np.zeros_like(a), where=b!=0), axis=1)
    return e



def normalized_data_bange(data):
    """Normalize the data, returning the normalized data, maximum value, and minimum value"""
    max_val = data.max(axis=0)
    min_val = data.min(axis=0)
    min_val = np.where(min_val > 4, 0, min_val)
    condition = np.logical_and(min_val < 3, min_val > 0)

    # 用 np.where 替换符合条件的元素
    min_val = np.where(condition, 0, min_val)
    normalized = (data - min_val) / (max_val - min_val)  # 线性归一化
    return normalized, max_val, min_val

def normalized_data(data):
    """Normalize the data, returning the normalized data, maximum value, and minimum value"""
    max_val = data.max(axis=0)
    min_val = data.min(axis=0)
    normalized = (data - min_val) / (max_val - min_val)  # 线性归一化
    return normalized, max_val, min_val


def load_data(filename,latent_vectors_name='latent_vectors'):
    book = xlrd.open_workbook(filename)
    sheet1 = book.sheet_by_name(latent_vectors_name)
    #Select parameters to obtain vectors generated by different models
    # vaesimple*:VAE
    # latent_vectors: cnn-vit


    sheet2 = book.sheet_by_name('parameters')
    sheet3 = book.sheet_by_name('bandgaps')

    m, n1, n2, n3 = sheet1.nrows, sheet1.ncols, sheet2.ncols, sheet3.ncols

    # Create arrays to store data
    lvs = np.zeros((m, n1))
    sps = np.zeros((m, n2))
    bgs = np.zeros((m, n3))

    # Read data from each worksheet
    for i in range(m):
        for j in range(n1):
            lvs[i, j] = sheet1.cell(i, j).value
        for j in range(n2):
            sps[i, j] = sheet2.cell(i, j).value
        for j in range(n3):
            bgs[i, j] = sheet3.cell(i, j).value

    # Normalize data
    lvs, lvs_max, lvs_min = normalized_data(lvs)
    sps, sps_max, sps_min = normalized_data(sps)
    bgs, bgs_max, bgs_min = normalized_data(bgs)

    # 将前249行的数据放入 group1，其余数据放入 group2  #组合式249  地下218
    group1_lvs = lvs[:249]
    group2_lvs = lvs[249:]

    group1_sps = sps[:249]
    group2_sps = sps[249:]

    group1_bgs = bgs[:249]
    group2_bgs = bgs[249:]

    # Dataset splitting
    tr_lvs, va_lvs, te_lvs = split_data(group1_lvs, group2_lvs)
    tr_sps, va_sps, te_sps = split_data(group1_sps, group2_sps)
    tr_bgs, va_bgs, te_bgs = split_data(group1_bgs, group2_bgs)

    return tr_lvs, va_lvs, te_lvs, tr_sps, va_sps, te_sps, tr_bgs, va_bgs, te_bgs, lvs_max, lvs_min, sps_max, sps_min, bgs_max, bgs_min


def split_data(group1, group2):
    """Split the dataset into training, validation, and test sets based on the number of data points in the two groups"""
    m1 = group1.shape[0]
    m2 = group2.shape[0]

    # Split group1
    tr_data1 = group1[:int(m1 * 0.90)]  # 90% of data for training
    va_data1 = group1[int(m1 * 0.90):int(m1 * 0.95)]  # 5% of data for training
    te_data1 = group1[int(m1 * 0.95):]  # 5% of data for training

    # Split group2
    tr_data2 = group2[:int(m2 * 0.90)]  # 90% of data for training
    va_data2 = group2[int(m2 * 0.90):int(m2 * 0.95)]  # 5% of data for training
    te_data2 = group2[int(m2 * 0.95):]  # 5% of data for training

    # Concatenate datasets
    tr_data = np.vstack((tr_data1, tr_data2))
    va_data = np.vstack((va_data1, va_data2))
    te_data = np.vstack((te_data1, te_data2))

    return tr_data, va_data, te_data





def training_model(dataset_path,latent_vectors_name,savemodel_name):
    tr_lvs, va_lvs, te_lvs, tr_sps, va_sps, te_sps, tr_bgs, va_bgs, te_bgs, lvs_max, lvs_min, sps_max, sps_min, bgs_max, bgs_min = load_data(
        dataset_path,latent_vectors_name)

    #lvs latent_vectors
    # sps sms_parameters
    # bgs bandgaps

    # Choose different datasets
    # training_model()
    # ..\\dataset\\dataset.xlsx         Integration of bandgap data for models with ground-based components
    # ..\\dataset\\no_ground-based_dataset.xlsx  Integration of bandgap data for models without ground-based components


    # Convert data to PyTorch Tensors
    tr_lvs = torch.FloatTensor(tr_lvs)
    va_lvs = torch.FloatTensor(va_lvs)
    te_lvs = torch.FloatTensor(te_lvs)

    tr_sps = torch.FloatTensor(tr_sps)
    va_sps = torch.FloatTensor(va_sps)
    te_sps = torch.FloatTensor(te_sps)

    tr_bgs = torch.FloatTensor(tr_bgs)
    va_bgs = torch.FloatTensor(va_bgs)
    te_bgs = torch.FloatTensor(te_bgs)


    # Create model, loss function, and optimizer
    model = CombinedMLP((tr_lvs.size(1)+tr_sps.size(1)),use_kanmlp=True,out_features=2)
    criterion = nn.MSELoss()  # Mean Squared Error Loss
    optimizer = optim.Adam(model.parameters(), lr=1e-3)

    tr_acc = []
    va_acc = []
    te_acc = []

    loss_history_train = []
    loss_history_validation = []

    for epoch in range(20000):
        model.train()  # Set model to training mode
        optimizer.zero_grad()   # Reset gradients

        # Forward pass
        x = torch.cat((tr_lvs, tr_sps), dim=1)
        outputs = model(x)

        # Calculate loss
        loss = criterion(outputs, tr_bgs)
        loss.backward()  # Backward pass
        optimizer.step()   # Update parameters

        # Record training and validation loss every 1 / 10 ... epoch
        if epoch % 1 == 0:
            model.eval()  # Set model to evaluation mode
            with torch.no_grad():
                # Calculate training loss
                outputs_train = model(torch.cat((tr_lvs, tr_sps), dim=1))
                loss_train_value = criterion(outputs_train, tr_bgs).item()
                loss_history_train.append(loss_train_value)

                # Calculate validation loss
                outputs_validation = model(torch.cat((va_lvs, va_sps), dim=1))
                loss_validation_value = criterion(outputs_validation, va_bgs).item()
                loss_history_validation.append(loss_validation_value)

                # Calculate accuracy
                tr_acc_value = calculate_accuracy(outputs_train*(bgs_max-bgs_min)+bgs_min, tr_bgs*(bgs_max-bgs_min)+bgs_min)
                va_acc_value = calculate_accuracy(outputs_validation*(bgs_max-bgs_min)+bgs_min, va_bgs*(bgs_max-bgs_min)+bgs_min)

                te_outputs = model(torch.cat((te_lvs, te_sps), dim=1))
                te_acc_value = calculate_accuracy(te_outputs*(bgs_max-bgs_min)+bgs_min, te_bgs*(bgs_max-bgs_min)+bgs_min)

                tr_acc.append(tr_acc_value.item())
                va_acc.append(va_acc_value.item())
                te_acc.append(te_acc_value.item())

                print(f"epoch {epoch} tr_acc: {tr_acc[-1]} loss: {loss_train_value}")
                print(f"epoch {epoch} va_acc: {va_acc[-1]} loss: {loss_validation_value}")
                print(f"epoch {epoch} te_acc: {te_acc[-1]} ")
        torch.save(model.state_dict(),
                   f'..\\newMoudle\\{savemodel_name}.pth')
        # Save the model
        if va_acc[-1] >= np.max(va_acc) and np.max(va_acc) > 0.98 :
            torch.save(model.state_dict(),
                       f'..\\newMoudle\\{savemodel_name}.pth')
            print("saved!")
            print(f"epoch {epoch} tr_acc: {tr_acc[-1]} loss: {loss_train_value}")
            print(f"epoch {epoch} va_acc: {va_acc[-1]} loss: {loss_validation_value}")
            print(f"epoch {epoch} te_acc: {te_acc[-1]} ")





if __name__ == '__main__':
    training_model("..\\dataset\\dataset.xlsx","latent_vectors",'TEST_1')

    # Choose different datasets
    # training_model()
    # ..\\dataset\\dataset.xlsx         Integration of bandgap data for models with ground-based components
    # ..\\dataset\\no_ground-based_dataset.xlsx  Integration of bandgap data for models without ground-based components


    # Select parameters to obtain vectors generated by different models
    # vaesimple*:VAE
    # latent_vectors: cnn-vit






